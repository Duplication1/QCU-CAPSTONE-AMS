# Linear Regression Formula Reference Card

## ğŸ“ Core Linear Regression Equations

### Basic Model
```
y = mx + b

where:
  y = predicted value (dependent variable)
  x = time period or independent variable
  m = slope (rate of change)
  b = y-intercept (starting value)
```

---

## ğŸ§® Calculating Regression Coefficients

### Given Data Points
```
x = [xâ‚, xâ‚‚, xâ‚ƒ, ..., xâ‚™]
y = [yâ‚, yâ‚‚, yâ‚ƒ, ..., yâ‚™]
n = number of data points
```

### Slope (m)
```
         nÂ·Î£(xáµ¢yáµ¢) - Î£xáµ¢Â·Î£yáµ¢
m = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         nÂ·Î£(xáµ¢Â²) - (Î£xáµ¢)Â²


Example:
  x = [0, 1, 2, 3, 4]
  y = [100, 95, 87, 82, 75]
  n = 5
  
  Î£x = 10
  Î£y = 439
  Î£(xy) = 795
  Î£(xÂ²) = 30
  
  m = (5Ã—795 - 10Ã—439) / (5Ã—30 - 10Â²)
  m = (3975 - 4390) / (150 - 100)
  m = -415 / 50
  m = -8.3
  
  Interpretation: Losing 8.3 points per time period
```

### Intercept (b)
```
     Î£yáµ¢ - mÂ·Î£xáµ¢
b = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          n


Example (using m = -8.3 from above):
  b = (439 - (-8.3)Ã—10) / 5
  b = (439 + 83) / 5
  b = 522 / 5
  b = 104.4
  
  Interpretation: Starting value at time 0
```

### Final Equation
```
y = -8.3x + 104.4

Predictions:
  Month 5: y = -8.3(5) + 104.4 = 62.9
  Month 6: y = -8.3(6) + 104.4 = 54.6
```

---

## ğŸ“Š R-Squared (RÂ²) - Coefficient of Determination

### Formula
```
       SS_res
RÂ² = 1 - â”€â”€â”€â”€â”€â”€â”€
       SS_tot

where:
  SS_res = Î£(yáµ¢ - Å·áµ¢)Â²     (Residual sum of squares)
  SS_tot = Î£(yáµ¢ - È³)Â²      (Total sum of squares)
  
  yáµ¢  = actual value
  Å·áµ¢  = predicted value (from regression line)
  È³   = mean of all y values
```

### Step-by-Step Calculation

```
1. Calculate mean of y:
   È³ = Î£yáµ¢ / n

2. Calculate predicted values using y = mx + b:
   Å·áµ¢ = mÂ·xáµ¢ + b

3. Calculate SS_tot (total variance):
   SS_tot = Î£(yáµ¢ - È³)Â²

4. Calculate SS_res (residual variance):
   SS_res = Î£(yáµ¢ - Å·áµ¢)Â²

5. Calculate RÂ²:
   RÂ² = 1 - (SS_res / SS_tot)
```

### Example
```
Data:
  x = [0, 1, 2, 3, 4]
  y = [100, 95, 87, 82, 75]
  
Regression: y = -8.3x + 104.4

Step 1: Mean
  È³ = 439 / 5 = 87.8

Step 2: Predicted values
  Å· = [104.4, 96.1, 87.8, 79.5, 71.2]

Step 3: SS_tot
  SS_tot = (100-87.8)Â² + (95-87.8)Â² + (87-87.8)Â² + (82-87.8)Â² + (75-87.8)Â²
  SS_tot = 148.84 + 51.84 + 0.64 + 33.64 + 163.84
  SS_tot = 398.8

Step 4: SS_res
  SS_res = (100-104.4)Â² + (95-96.1)Â² + (87-87.8)Â² + (82-79.5)Â² + (75-71.2)Â²
  SS_res = 19.36 + 1.21 + 0.64 + 6.25 + 14.44
  SS_res = 41.9

Step 5: RÂ²
  RÂ² = 1 - (41.9 / 398.8)
  RÂ² = 1 - 0.105
  RÂ² = 0.895
  
Interpretation: 89.5% of variance explained (excellent fit!)
```

---

## ğŸ¯ Asset Risk Score Formula

### Complete Formula
```
Risk Score = Base_Risk + Age_Factor + Issue_Factor + Degradation_Factor

where:
  Base_Risk = 100 - Condition_Score
  
  Condition_Score mapping:
    Excellent      â†’ 100
    Good           â†’ 75
    Fair           â†’ 50
    Poor           â†’ 25
    Non-Functional â†’ 0
  
  Age_Factor = min(30, (Age_Days / 1095) Ã— 30)
    - 1095 days = 3 years
    - Max contribution: 30 points
  
  Issue_Factor = min(40, Issue_Count Ã— 5)
    - Each issue adds 5 points
    - Max contribution: 40 points
  
  Degradation_Factor = min(30, Recent_Changes Ã— 10)
    - Recent_Changes = condition changes in last 90 days
    - Each change adds 10 points
    - Max contribution: 30 points
```

### Example Calculation
```
Asset: Desktop Computer
  Condition: Fair â†’ 50 points
  Age: 730 days (2 years)
  Issues: 3 tickets
  Recent Changes: 2 (last 90 days)

Base_Risk = 100 - 50 = 50

Age_Factor = min(30, (730/1095) Ã— 30)
           = min(30, 0.667 Ã— 30)
           = min(30, 20)
           = 20

Issue_Factor = min(40, 3 Ã— 5)
             = min(40, 15)
             = 15

Degradation_Factor = min(30, 2 Ã— 10)
                   = min(30, 20)
                   = 20

Risk Score = 50 + 20 + 15 + 20 = 105
           = Clamped to max 100
           
Final Risk Score: 100 (Critical)
```

### Risk Categories
```
Score â‰¥ 70  â†’ Critical (ğŸ”´)
Score â‰¥ 50  â†’ High     (ğŸŸ )
Score â‰¥ 30  â†’ Medium   (ğŸŸ¡)
Score < 30  â†’ Low      (ğŸŸ¢)
```

---

## ğŸ“ˆ Prediction Formula

### Single Point Prediction
```
y_predicted = m Ã— x_future + b

Example (using y = -8.3x + 104.4):
  Predict month 10:
  yâ‚â‚€ = -8.3 Ã— 10 + 104.4
  yâ‚â‚€ = -83 + 104.4
  yâ‚â‚€ = 21.4
```

### Multiple Predictions
```
For next 6 months (if last data point is x=12):
  
  Month 13: yâ‚â‚ƒ = mÃ—13 + b
  Month 14: yâ‚â‚„ = mÃ—14 + b
  Month 15: yâ‚â‚… = mÃ—15 + b
  Month 16: yâ‚â‚† = mÃ—16 + b
  Month 17: yâ‚â‚‡ = mÃ—17 + b
  Month 18: yâ‚â‚ˆ = mÃ—18 + b
```

---

## ğŸ”¢ Summary Statistics

### Mean (Average)
```
     Î£yáµ¢
È³ = â”€â”€â”€â”€â”€
      n
```

### Variance
```
         Î£(yáµ¢ - È³)Â²
Var(y) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            n-1
```

### Standard Deviation
```
Ïƒ = âˆšVar(y)
```

### Correlation Coefficient (r)
```
r = âˆšRÂ²   (if slope is positive)
r = -âˆšRÂ²  (if slope is negative)

Range: -1 â‰¤ r â‰¤ 1
  r = 1:  Perfect positive correlation
  r = -1: Perfect negative correlation
  r = 0:  No correlation
```

---

## ğŸ“ Quick Reference Table

| Symbol | Meaning |
|--------|---------|
| m | Slope (rate of change) |
| b | Y-intercept (starting value) |
| RÂ² | Coefficient of determination (model fit) |
| n | Number of data points |
| Î£ | Sum of all values |
| xáµ¢ | Individual x value |
| yáµ¢ | Individual y value |
| Å·áµ¢ | Predicted y value |
| È³ | Mean of y values |
| SS_res | Sum of squared residuals |
| SS_tot | Total sum of squares |

---

## ğŸ’¡ Interpretation Guide

### Slope (m)
- **m > 0**: Positive trend (improving)
- **m < 0**: Negative trend (degrading)
- **m â‰ˆ 0**: No clear trend (stable)
- **|m|** = magnitude of change per time unit

### RÂ² Score
- **0.9-1.0**: Excellent fit
- **0.7-0.9**: Good fit
- **0.5-0.7**: Moderate fit
- **0.3-0.5**: Weak fit
- **0.0-0.3**: Very weak fit

### Confidence in Predictions
```
High Confidence: RÂ² > 0.8 AND n > 10
Medium Confidence: RÂ² > 0.6 AND n > 6
Low Confidence: RÂ² < 0.6 OR n < 6
```

---

## ğŸ“ Mathematical Properties

### Least Squares Property
The regression line minimizes the sum of squared vertical distances from points to the line.

### Why Square the Residuals?
- Prevents positive and negative errors from canceling
- Penalizes larger errors more heavily
- Mathematically tractable (has a unique solution)

### Assumptions
1. Linear relationship between x and y
2. Independence of observations
3. Homoscedasticity (constant variance)
4. Normal distribution of residuals (for inference)

---

**Note**: All formulas implemented in `controller/get_predictive_analytics.php`

**References**:
- Statistics textbooks
- Linear algebra resources
- Machine learning fundamentals
